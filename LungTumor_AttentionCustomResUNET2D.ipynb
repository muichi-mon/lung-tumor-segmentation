{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5905519,"sourceType":"datasetVersion","datasetId":3391284},{"sourceId":13494645,"sourceType":"datasetVersion","datasetId":8562918}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/muichimon/lungtumor-attentioncustomresunet2d?scriptVersionId=270711806\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# **# 00 - set up**","metadata":{}},{"cell_type":"code","source":"%pip install torchio --q\n%pip install monai --q\n%pip install celluloid --q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T07:45:33.252266Z","iopub.execute_input":"2025-10-25T07:45:33.252466Z","iopub.status.idle":"2025-10-25T07:47:04.630755Z","shell.execute_reply.started":"2025-10-25T07:45:33.25245Z","shell.execute_reply":"2025-10-25T07:47:04.629947Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%matplotlib notebook\n    \nfrom pathlib import Path\nimport nibabel as nib\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom tqdm.notebook import tqdm\nfrom torch.utils.data import DataLoader\n\nfrom celluloid import Camera\nfrom IPython.display import HTML\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T07:47:04.631931Z","iopub.execute_input":"2025-10-25T07:47:04.63227Z","iopub.status.idle":"2025-10-25T07:47:05.543579Z","shell.execute_reply.started":"2025-10-25T07:47:04.632236Z","shell.execute_reply":"2025-10-25T07:47:05.543055Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport torch\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)  \n\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False  \n\nset_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T07:47:05.54427Z","iopub.execute_input":"2025-10-25T07:47:05.544478Z","iopub.status.idle":"2025-10-25T07:47:05.555426Z","shell.execute_reply.started":"2025-10-25T07:47:05.54446Z","shell.execute_reply":"2025-10-25T07:47:05.554786Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **set root paths**","metadata":{}},{"cell_type":"code","source":"train_path = Path(\"/kaggle/input/lung-cancer-segment/train/\")\nval_path = Path(\"/kaggle/input/lung-cancer-segment/val/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T07:47:05.557503Z","iopub.execute_input":"2025-10-25T07:47:05.557706Z","iopub.status.idle":"2025-10-25T07:47:05.567661Z","shell.execute_reply.started":"2025-10-25T07:47:05.55769Z","shell.execute_reply":"2025-10-25T07:47:05.567067Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# understanding single slice\nimg = np.load(train_path/\"0\"/\"data\"/\"100.npy\")\nprint(img.shape)\n\nfig = plt.figure()\nplt.imshow(img, cmap=\"bone\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T07:47:05.56833Z","iopub.execute_input":"2025-10-25T07:47:05.56855Z","execution_failed":"2025-10-25T07:49:42.053Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_sorted_slice_paths(folder: Path):\n    files = sorted(folder.glob(\"*.npy\"), key=lambda f: int(f.stem))\n    if not files:\n        raise ValueError(f\"No .npy slices found in {folder}\")\n    return files\n\ndef get_patient_path(patient_id: str, train: bool) -> Path:\n    if train:\n        return train_path / str(patient_id)\n    else:\n        return val_path / str(patient_id)\n\ndef get_img_path(patient_path: Path) -> Path:\n    img_path = patient_path / \"data\"\n    if not img_path.exists():\n        raise FileNotFoundError(f\"Image folder not found: {img_path}\")\n    return img_path\n\ndef get_label_path(patient_path: Path) -> Path:\n    label_path = patient_path / \"masks\"\n    if not label_path.exists():\n        raise FileNotFoundError(f\"Label folder not found: {label_path}\")\n    return label_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T07:47:05.900659Z","iopub.execute_input":"2025-10-25T07:47:05.900924Z","iopub.status.idle":"2025-10-25T07:47:05.90806Z","shell.execute_reply.started":"2025-10-25T07:47:05.9009Z","shell.execute_reply":"2025-10-25T07:47:05.907343Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **visualization**","metadata":{}},{"cell_type":"code","source":"patient_path = get_patient_path(\"0\", True)\n\nimg_path = get_img_path(patient_path)\nlabel_path = get_label_path(patient_path)\n\nsorted_img_files = get_sorted_slice_paths(img_path)\nsorted_mask_files = get_sorted_slice_paths(label_path)\n\nimg_volume = np.stack([np.load(f) for f in sorted_img_files], axis=-1).astype(np.float32)\nmask_volume = np.stack([np.load(f) for f in sorted_mask_files], axis=-1).astype(np.uint8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T07:47:05.908859Z","iopub.execute_input":"2025-10-25T07:47:05.909175Z","iopub.status.idle":"2025-10-25T07:47:11.030325Z","shell.execute_reply.started":"2025-10-25T07:47:05.909151Z","shell.execute_reply":"2025-10-25T07:47:11.029755Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig = plt.figure()\ncamera = Camera(fig)\n\nfor i in range(img_volume.shape[2]):\n    \n    plt.imshow(img_volume[:, :, i], cmap=\"bone\")\n    mask_ = np.ma.masked_where(mask_volume[:, :, i] == 0, mask_volume[:, :, i])\n    plt.imshow(mask_, cmap=\"autumn\", alpha=0.5)\n    \n    camera.snap();\n\nanimation = camera.animate();\nHTML(animation.to_html5_video())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T07:47:11.031087Z","iopub.execute_input":"2025-10-25T07:47:11.03133Z","iopub.status.idle":"2025-10-25T07:48:02.728461Z","shell.execute_reply.started":"2025-10-25T07:47:11.031307Z","shell.execute_reply":"2025-10-25T07:48:02.727635Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **# 01 - data set**","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass LungTumorDataset(Dataset):\n    \n    \"\"\"\n    Returns individual 2D slices (img, mask) from 3D CT volumes.\n    \"\"\"\n\n    def __init__(self, patient_ids, train=True, transform=None):\n        \"\"\"\n        patient_ids : list of patient identifiers (str or int)\n        train       : bool, whether to use train or val dataset\n        transform   : optional, function to apply data augmentation (img, mask)\n        \"\"\"\n        self.samples = []\n        self.train = train\n        self.transform = transform\n\n        # Build a list of (patient_id, slice_idx)\n        for pid in patient_ids:\n            patient_path = get_patient_path(pid, self.train)\n            img_folder = get_img_path(patient_path)\n            num_slices = len(get_sorted_slice_paths(img_folder))\n            for slice_idx in range(30, num_slices): # remove first 30 slices\n                self.samples.append((pid, slice_idx))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        pid, slice_idx = self.samples[idx]\n\n        patient_path = get_patient_path(pid, self.train)\n        img_folder = get_img_path(patient_path)\n        mask_folder = get_label_path(patient_path)\n\n        img_file = get_sorted_slice_paths(img_folder)[slice_idx]\n        mask_file = get_sorted_slice_paths(mask_folder)[slice_idx]\n\n        img = np.load(img_file).astype(np.float32)\n        mask = np.load(mask_file).astype(np.uint8)\n\n         # convert 2D slice to fake 3D volume (C,H,W,D)\n        img_volume = torch.tensor(img).unsqueeze(0).unsqueeze(-1)   # (1,H,W,1)\n        mask_volume = torch.tensor(mask).unsqueeze(0).unsqueeze(-1) # (1,H,W,1)\n\n        if self.transform:\n            # TorchIO expects a Subject\n            subject = tio.Subject(\n                image=tio.ScalarImage(tensor=img_volume),\n                mask=tio.LabelMap(tensor=mask_volume)\n            )\n            transformed = self.transform(subject)\n            img_volume = transformed['image'].data\n            mask_volume = transformed['mask'].data\n\n        # squeeze back to 2D (C,H,W)\n        img_tensor = img_volume.squeeze(-1)\n        mask_tensor = mask_volume.squeeze(-1)\n\n        return img_tensor, mask_tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T07:48:02.729306Z","iopub.execute_input":"2025-10-25T07:48:02.729791Z","iopub.status.idle":"2025-10-25T07:48:02.737551Z","shell.execute_reply.started":"2025-10-25T07:48:02.729767Z","shell.execute_reply":"2025-10-25T07:48:02.736759Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **define transforms**","metadata":{}},{"cell_type":"code","source":"import torchio as tio\n\nprocess = tio.Compose([\n    tio.ToCanonical(),               # fix orientation\n    tio.RescaleIntensity((-1,1)),    # normalize intensity\n    tio.CropOrPad((256,256,1))       # crop/pad HxWxD=1\n])\n\naugmentation = tio.RandomAffine(scales=(0.9,1.1), degrees=(-20,20), translation=5)\n\ntrain_transform = tio.Compose([process, augmentation])\nval_transform = tio.Compose([process])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T07:48:02.7384Z","iopub.execute_input":"2025-10-25T07:48:02.738634Z","iopub.status.idle":"2025-10-25T07:48:03.665918Z","shell.execute_reply.started":"2025-10-25T07:48:02.738617Z","shell.execute_reply":"2025-10-25T07:48:03.665162Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ids = list(range(57))\nval_ids = list(range(57, 63))\n\ntrain_dataset = LungTumorDataset(train_ids, train=True, transform=train_transform)\nval_dataset   = LungTumorDataset(val_ids, train=False, transform=val_transform)\n\nprint(len(val_dataset), len(train_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T07:48:03.666705Z","iopub.execute_input":"2025-10-25T07:48:03.667167Z","iopub.status.idle":"2025-10-25T07:48:04.414709Z","shell.execute_reply.started":"2025-10-25T07:48:03.667141Z","shell.execute_reply":"2025-10-25T07:48:04.414078Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **# 03 - model**","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass DoubleConv2D(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.conv_block = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n            nn.ReLU(inplace=False),   \n            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n            nn.ReLU(inplace=False)    \n        )\n        # Match dimensions for residual connection\n        self.residual_conv = None\n        if in_ch != out_ch:\n            self.residual_conv = nn.Conv2d(in_ch, out_ch, kernel_size=1)\n\n    def forward(self, x):\n        residual = x\n        out = self.conv_block(x)\n        \n        if self.residual_conv is not None:\n            residual = self.residual_conv(residual)\n        \n        out = out + residual  \n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T07:48:18.896786Z","iopub.execute_input":"2025-10-25T07:48:18.897085Z","iopub.status.idle":"2025-10-25T07:48:18.901989Z","shell.execute_reply.started":"2025-10-25T07:48:18.897063Z","shell.execute_reply":"2025-10-25T07:48:18.901274Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass AttentionGate2D(nn.Module):\n    def __init__(self, F_g, F_l, F_int):\n        \"\"\"\n        F_g: number of channels in the gating (decoder) signal\n        F_l: number of channels in the skip connection (encoder)\n        F_int: number of intermediate channels (usually smaller)\n        \"\"\"\n        super().__init__()\n        self.W_g = nn.Sequential(\n            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(F_int)\n        )\n\n        self.W_x = nn.Sequential(\n            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(F_int)\n        )\n\n        self.psi = nn.Sequential(\n            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(1),\n            nn.Sigmoid()\n        )\n\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x, g):\n        \"\"\"\n        x: encoder feature map (skip connection)\n        g: decoder feature map (gating signal)\n        \"\"\"\n        g1 = self.W_g(g)\n        x1 = self.W_x(x)\n        psi = self.relu(g1 + x1)\n        psi = self.psi(psi)\n        return x * psi  # element-wise attention","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T07:48:22.50253Z","iopub.execute_input":"2025-10-25T07:48:22.503057Z","iopub.status.idle":"2025-10-25T07:48:22.509123Z","shell.execute_reply.started":"2025-10-25T07:48:22.503035Z","shell.execute_reply":"2025-10-25T07:48:22.508317Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class UNet2D_Attention(nn.Module):\n    def __init__(self, in_channels=1, out_channels=1):\n        super().__init__()\n\n        # Encoder\n        self.layer1 = DoubleConv2D(in_channels, 32)\n        self.layer2 = DoubleConv2D(32, 64)\n        self.layer3 = DoubleConv2D(64, 128)\n        self.layer4 = DoubleConv2D(128, 256)\n\n        # Attention Gates\n        self.att3 = AttentionGate2D(F_g=128, F_l=128, F_int=64)\n        self.att2 = AttentionGate2D(F_g=64, F_l=64, F_int=32)\n        self.att1 = AttentionGate2D(F_g=32, F_l=32, F_int=16)\n\n        # Decoder\n        self.upconv1 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n        self.layer5 = DoubleConv2D(128 + 128, 128)\n\n        self.upconv2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n        self.layer6 = DoubleConv2D(64 + 64, 64)\n\n        self.upconv3 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n        self.layer7 = DoubleConv2D(32 + 32, 32)\n\n        # Output\n        self.layer8 = nn.Conv2d(32, out_channels, kernel_size=1)\n\n        # Pooling\n        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n    def forward(self, x):\n        # Encoder\n        x1 = self.layer1(x)                 # (32)\n        x2 = self.layer2(self.maxpool(x1))  # (64)\n        x3 = self.layer3(self.maxpool(x2))  # (128)\n        x4 = self.layer4(self.maxpool(x3))  # (256)\n\n        # Decoder level 1\n        x5 = self.upconv1(x4)               # (128)\n        x3 = self.att3(x3, x5)              # attention on skip from encoder\n        x5 = torch.cat([x5, x3], dim=1)\n        x5 = self.layer5(x5)\n\n        # Decoder level 2\n        x6 = self.upconv2(x5)               # (64)\n        x2 = self.att2(x2, x6)\n        x6 = torch.cat([x6, x2], dim=1)\n        x6 = self.layer6(x6)\n\n        # Decoder level 3\n        x7 = self.upconv3(x6)               # (32)\n        x1 = self.att1(x1, x7)\n        x7 = torch.cat([x7, x1], dim=1)\n        x7 = self.layer7(x7)\n\n        out = self.layer8(x7)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T07:48:22.820987Z","iopub.execute_input":"2025-10-25T07:48:22.821254Z","iopub.status.idle":"2025-10-25T07:48:22.829628Z","shell.execute_reply.started":"2025-10-25T07:48:22.821234Z","shell.execute_reply":"2025-10-25T07:48:22.828797Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = UNet2D_Attention()\n\nrandom_input = torch.randn(1, 1, 256, 256)\noutput = model(random_input)\nassert output.shape == random_input.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T07:48:37.214592Z","iopub.execute_input":"2025-10-25T07:48:37.214903Z","iopub.status.idle":"2025-10-25T07:48:37.875902Z","shell.execute_reply.started":"2025-10-25T07:48:37.214852Z","shell.execute_reply":"2025-10-25T07:48:37.874883Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **# 04 - train**","metadata":{}},{"cell_type":"markdown","source":"## **sampler**","metadata":{}},{"cell_type":"code","source":"target_list = []\n\nfor _, label in tqdm(train_dataset):\n    if label.any():\n        target_list.append(1)\n    else:\n        target_list.append(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:59:02.387611Z","iopub.execute_input":"2025-10-23T12:59:02.388458Z","iopub.status.idle":"2025-10-23T13:07:43.854296Z","shell.execute_reply.started":"2025-10-23T12:59:02.38843Z","shell.execute_reply":"2025-10-23T13:07:43.853549Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import WeightedRandomSampler\n\ntarget_list = np.array(target_list)\n\n# Compute class counts\nclass_counts = np.bincount(target_list)   # [count_0, count_1]\nclass_weights = 1. / class_counts         # inverse of frequency\n\n# Assign each sample a weight\nsample_weights = [class_weights[t] for t in target_list]\nsample_weights = torch.DoubleTensor(sample_weights)\n\n# Create sampler\nsampler = WeightedRandomSampler(\n    sample_weights,\n    num_samples=len(sample_weights),  # you can choose fewer if you want\n    replacement=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T13:07:43.855056Z","iopub.execute_input":"2025-10-23T13:07:43.855319Z","iopub.status.idle":"2025-10-23T13:07:43.864693Z","shell.execute_reply.started":"2025-10-23T13:07:43.855295Z","shell.execute_reply":"2025-10-23T13:07:43.864031Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **dataloader**","metadata":{}},{"cell_type":"code","source":"train_loader = DataLoader(\n    train_dataset, \n    sampler=sampler,\n    num_workers=2, \n    batch_size=8\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T13:07:43.865436Z","iopub.execute_input":"2025-10-23T13:07:43.865684Z","iopub.status.idle":"2025-10-23T13:07:43.869296Z","shell.execute_reply.started":"2025-10-23T13:07:43.865659Z","shell.execute_reply":"2025-10-23T13:07:43.868754Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_loader = DataLoader(\n    val_dataset, \n    num_workers=2, \n    batch_size=8\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T07:48:53.329566Z","iopub.execute_input":"2025-10-25T07:48:53.330171Z","iopub.status.idle":"2025-10-25T07:48:53.333856Z","shell.execute_reply.started":"2025-10-25T07:48:53.330146Z","shell.execute_reply":"2025-10-25T07:48:53.333095Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(val_loader), len(train_loader))\n\nbatch = next(iter(val_loader))\nprint(type(batch)) \n\nimgs, masks = batch\nprint(imgs.shape, masks.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T13:47:57.469402Z","iopub.execute_input":"2025-10-23T13:47:57.469664Z","iopub.status.idle":"2025-10-23T13:47:57.919216Z","shell.execute_reply.started":"2025-10-23T13:47:57.469642Z","shell.execute_reply":"2025-10-23T13:47:57.918452Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **pytorch lightning model**","metadata":{}},{"cell_type":"code","source":"import pytorch_lightning as pl\n\nclass LungTumorModel(pl.LightningModule):\n    def __init__(self, lr=1e-4):\n        super().__init__()\n        self.model = UNet2D_Attention()\n        self.lr = lr\n        self.loss_fn = nn.BCEWithLogitsLoss()\n\n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        imgs, masks = batch\n        preds = self(imgs)\n        loss = self.loss_fn(preds.float(), masks.float())\n        self.log(\"train_loss\", loss, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        imgs, masks = batch\n        preds = self(imgs)\n        loss = self.loss_fn(preds.float(), masks.float())\n        self.log(\"val_loss\", loss, prog_bar=True)\n        return loss\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, mode='min', factor=0.5, patience=3\n        )\n        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T07:49:20.35951Z","iopub.execute_input":"2025-10-25T07:49:20.359744Z","iopub.status.idle":"2025-10-25T07:49:20.366304Z","shell.execute_reply.started":"2025-10-25T07:49:20.359727Z","shell.execute_reply":"2025-10-25T07:49:20.365509Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = LungTumorModel()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T07:49:22.056628Z","iopub.execute_input":"2025-10-25T07:49:22.057151Z","iopub.status.idle":"2025-10-25T07:49:22.082796Z","shell.execute_reply.started":"2025-10-25T07:49:22.057127Z","shell.execute_reply":"2025-10-25T07:49:22.082263Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pytorch_lightning.callbacks import ModelCheckpoint \n\ncheckpoint_callback = ModelCheckpoint(\n    monitor='val_loss',        \n    save_top_k=1,        \n    mode='min',\n    filename=\"best_model\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T07:49:24.630574Z","iopub.execute_input":"2025-10-25T07:49:24.631082Z","iopub.status.idle":"2025-10-25T07:49:24.647834Z","shell.execute_reply.started":"2025-10-25T07:49:24.631059Z","shell.execute_reply":"2025-10-25T07:49:24.647282Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pytorch_lightning.loggers import TensorBoardLogger\n\ntrainer = pl.Trainer(\n    accelerator=\"auto\",\n    devices=\"auto\",\n    logger=TensorBoardLogger(save_dir=\"logs\"),\n    log_every_n_steps=10,\n    callbacks=checkpoint_callback,\n    max_epochs=30\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T07:49:26.948526Z","iopub.execute_input":"2025-10-25T07:49:26.948766Z","iopub.status.idle":"2025-10-25T07:49:27.018376Z","shell.execute_reply.started":"2025-10-25T07:49:26.948749Z","shell.execute_reply":"2025-10-25T07:49:27.017852Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.fit(model, train_loader, val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T14:02:19.211602Z","iopub.execute_input":"2025-10-23T14:02:19.211844Z","iopub.status.idle":"2025-10-23T14:02:21.593121Z","shell.execute_reply.started":"2025-10-23T14:02:19.211828Z","shell.execute_reply":"2025-10-23T14:02:21.592039Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **# 05 - evaluation**","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# model = LungTumorModel.load_from_checkpoint(\"/kaggle/input/checkpoints/LungTumor_BCE_CustomUNET2D.ckpt\")\nmodel.to(device);\nmodel.freeze();\nmodel.eval();","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T07:49:34.74191Z","iopub.execute_input":"2025-10-25T07:49:34.742176Z","iopub.status.idle":"2025-10-25T07:49:35.004474Z","shell.execute_reply.started":"2025-10-25T07:49:34.742157Z","shell.execute_reply":"2025-10-25T07:49:35.003912Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **iou sensitivity specificty**","metadata":{}},{"cell_type":"code","source":"def compute_iou_sens_spec(preds, targets, threshold=0.5, eps=1e-6):\n    \"\"\"\n    Compute mean IoU, sensitivity, and specificity over a batch of predictions and targets.\n\n    Args:\n        preds (torch.Tensor): Model outputs, shape [B, 1, H, W]\n        targets (torch.Tensor): Ground truth masks, shape [B, 1, H, W]\n        threshold (float): Binarization threshold for predictions\n        eps (float): Small epsilon to prevent division by zero\n\n    Returns:\n        dict: {'iou': float, 'sensitivity': float, 'specificity': float}\n    \"\"\"\n    # Binarize predictions\n    preds = (preds > threshold).float()\n\n    # Flatten spatial dimensions\n    preds_flat = preds.view(preds.size(0), -1)\n    targets_flat = targets.view(targets.size(0), -1)\n\n    # True positives, false positives, false negatives, true negatives\n    TP = (preds_flat * targets_flat).sum(dim=1)\n    FP = (preds_flat * (1 - targets_flat)).sum(dim=1)\n    FN = ((1 - preds_flat) * targets_flat).sum(dim=1)\n    TN = ((1 - preds_flat) * (1 - targets_flat)).sum(dim=1)\n\n    # IoU\n    iou = (TP + eps) / (TP + FP + FN + eps)\n\n    # Sensitivity (Recall)\n    sensitivity = (TP + eps) / (TP + FN + eps)\n\n    # Specificity\n    specificity = (TN + eps) / (TN + FP + eps)\n\n    # Return mean across batch\n    return {\n        'iou': iou.mean().item(),\n        'sensitivity': sensitivity.mean().item(),\n        'specificity': specificity.mean().item()\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T07:49:37.322383Z","iopub.execute_input":"2025-10-25T07:49:37.322632Z","iopub.status.idle":"2025-10-25T07:49:37.328729Z","shell.execute_reply.started":"2025-10-25T07:49:37.322615Z","shell.execute_reply":"2025-10-25T07:49:37.328176Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"total_iou = 0\ntotal_sens = 0\ntotal_spec = 0\nnum_batches = 0\n\nwith torch.no_grad():\n    for images, masks in tqdm(val_loader):\n        \n        images = images.to(device)\n        masks = masks.to(device)\n        \n        outputs = model(images)\n        metrics = compute_iou_sens_spec(outputs.float(), masks.float())\n\n        total_iou += metrics['iou']\n        total_sens += metrics['sensitivity']\n        total_spec += metrics['specificity']\n        num_batches += 1\n\nmean_iou = total_iou / num_batches\nmean_sens = total_sens / num_batches\nmean_spec = total_spec / num_batches\n\nprint(f\"Mean IoU: {mean_iou:.4f}\")\nprint(f\"Mean Sensitivity: {mean_sens:.4f}\")\nprint(f\"Mean Specificity: {mean_spec:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T07:49:49.050053Z","iopub.execute_input":"2025-10-25T07:49:49.050625Z","iopub.status.idle":"2025-10-25T07:49:49.119589Z","shell.execute_reply.started":"2025-10-25T07:49:49.050601Z","shell.execute_reply":"2025-10-25T07:49:49.118611Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **result visualization**","metadata":{}},{"cell_type":"code","source":"patient_path = get_patient_path(\"57\", False)\nimg_path = get_img_path(patient_path)\nlabel_path = get_label_path(patient_path)\n\nsorted_img_files = get_sorted_slice_paths(img_path)\nsorted_mask_files = get_sorted_slice_paths(label_path)\n\nimg_volume = np.stack([np.load(f) for f in sorted_img_files], axis=-1).astype(np.float32)  # (H, W, D)\nmask_volume = np.stack([np.load(f) for f in sorted_mask_files], axis=-1).astype(np.float32)\n\n# --------------------------\n# Wrap in TorchIO Subject\n# --------------------------\nsubject = tio.Subject(\n    image=tio.ScalarImage(tensor=torch.from_numpy(img_volume).unsqueeze(0)),  # (1, H, W, D)\n    mask=tio.LabelMap(tensor=torch.from_numpy(mask_volume).unsqueeze(0))\n)\n\n# --------------------------\n# Apply validation transform\n# --------------------------\nval_transform = tio.Compose([\n    tio.ToCanonical(),\n    tio.RescaleIntensity((-1, 1)),\n])\n\n\nsubject = val_transform(subject)\nimg_volume = subject.image.data      # shape (1, 256, 256, D)\nmask_volume = subject.mask.data      # shape (1, 256, 256, D)\n\n# --------------------------\n# Reorder for model: (D, 1, H, W)\n# --------------------------\nimg_volume = img_volume.permute(3, 0, 1, 2).contiguous()  # (D, 1, H, W)\nmask_volume = mask_volume.permute(3, 0, 1, 2).contiguous()\n\nimg_volume = img_volume.to(device)\nmask_volume = mask_volume.to(device)\n\nprint(\"Prepared volume shape:\", img_volume.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T12:30:48.749999Z","iopub.execute_input":"2025-10-24T12:30:48.750294Z","iopub.status.idle":"2025-10-24T12:30:50.694467Z","shell.execute_reply.started":"2025-10-24T12:30:48.750274Z","shell.execute_reply":"2025-10-24T12:30:50.693766Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"chunk_size = 50\npreds_list = []\n\n# Iterate over the depth dimension (0th axis)\nfor start in range(0, img_volume.shape[0], chunk_size):\n    end = min(start + chunk_size, img_volume.shape[0])\n    \n    # Slice depth chunk â†’ shape [chunk, 1, H, W]\n    chunk = img_volume[start:end]  \n    \n    # Move to device\n    chunk = chunk.to(device).float()\n    \n    with torch.no_grad():\n        preds_chunk = torch.sigmoid(model(chunk))  # [chunk, 1, H, W]\n    \n    preds_list.append(preds_chunk.cpu())  # move to CPU immediately\n\n# Concatenate predictions along depth axis\npreds = torch.cat(preds_list, dim=0)  # shape: [D, 1, H, W]\n\n# Binarize\npreds_bin = (preds > 0.5).float()\n\nprint(\"Unique values in preds_bin:\", torch.unique(preds_bin))\n\n# Rearrange to (H, W, D)\npred_mask = preds_bin.permute(2, 3, 0, 1).squeeze(-1)  # (H, W, D)\n\n# Move image to same shape for visualization\nimg_volume_np = img_volume.permute(2, 3, 0, 1).squeeze(-1).squeeze(1).cpu().numpy()\nmask_np = mask_volume.squeeze(1).permute(1, 2, 0).cpu().numpy()\npred_mask_np = pred_mask.cpu().numpy() \n\nprint(\"Shapes:\")\nprint(\"img_volume:\", img_volume_np.shape)\nprint(\"mask_volume:\", mask_np.shape)\nprint(\"pred_mask:\", pred_mask_np.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T12:30:51.928866Z","iopub.execute_input":"2025-10-24T12:30:51.929139Z","iopub.status.idle":"2025-10-24T12:30:54.812823Z","shell.execute_reply.started":"2025-10-24T12:30:51.929118Z","shell.execute_reply":"2025-10-24T12:30:54.812117Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig = plt.figure()\ncamera = Camera(fig)\n\nfor i in range(img_volume_np.shape[2]):\n    \n    plt.imshow(img_volume_np[:, :, i], cmap=\"bone\")\n    \n    mask_ = np.ma.masked_where(mask_np[:, :, i] == 0, mask_np[:, :, i])\n    plt.imshow(mask_, cmap=\"autumn\", alpha=0.5)\n    \n    pred_ = np.ma.masked_where(pred_mask_np[:, :, i] == 0, pred_mask_np[:, :, i])\n    plt.imshow(pred_, cmap=\"winter\", alpha=0.5)\n    \n    camera.snap()\n\nanimation = camera.animate();\nHTML(animation.to_html5_video())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T12:30:56.847144Z","iopub.execute_input":"2025-10-24T12:30:56.847409Z","iopub.status.idle":"2025-10-24T12:31:28.483088Z","shell.execute_reply.started":"2025-10-24T12:30:56.847389Z","shell.execute_reply":"2025-10-24T12:31:28.482433Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}